# Junior‑aware secondary scoring rubric.
# Each criterion scored 0.0–1.0 in 0.1 increments.
# The model receives analyzer output (which now includes PASS / PARTIAL / FAIL and summaries).
# Philosophy:
# - Start from a generous baseline when core fundamentals appear (several PASS or PARTIAL).
# - PARTIAL in analyzer usually maps to mid band (0.4–0.7) not a near-fail.
# - Lack of explicit evidence => cautious mid (0.4–0.6), not punitive low, unless analyzer shows FAIL signals.

criteria:
  - id: features
    name: "Features work as intended"
    weight: 1
    prompt: |
      You are grading a BEGINNER Django project. Use ONLY analyzer output.
      Heuristics (junior-friendly):
        - Many PASS + few FAIL => assume core features likely run: 0.7–0.9 (maybe 1.0 if README + structure strong).
        - Mix of PASS and PARTIAL, sparse FAIL => 0.5–0.7.
        - Several FAIL in foundational areas (README missing + security fail + structure fail) => 0.2–0.4.
        - Overwhelming FAIL signals => 0.0–0.2.
        - If evidence thin but not negative: default to 0.5 (do NOT guess very high or very low).
      Allowed values: 0.0 0.1 ... 1.0 (step 0.1)
      Output strictly two lines:
      SCORE: <value>
      JUSTIFICATION: <concise reason referencing PASS/PARTIAL/FAIL counts>

  - id: code_quality
    name: "Code is clean, readable and well structured"
    weight: 1
    prompt: |
      Junior tolerance. Map analyzer checks:
        - If "Code is clean" PASS and complexity PASS (or PARTIAL) => 0.8–1.0.
        - If one PASS and others PARTIAL (no FAIL) => 0.6–0.8.
        - PARTIAL + PARTIAL (no FAIL) => 0.5–0.6.
        - Any single FAIL but rest PASS/PARTIAL => 0.3–0.5.
        - Multiple related FAIL (readability + complexity) => 0.0–0.3.
      Prefer mid (0.5–0.6) over very low if issues are typical beginner style, not catastrophic.
      Allowed: 0.0–1.0 step 0.1
      Format:
      SCORE: <value>
      JUSTIFICATION: <reason>

  - id: git_usage
    name: "Git has properly been used"
    weight: 1
    prompt: |
      Evaluate commit message quality + organization.
        - PASS on Git messages => 0.7–0.9 (bump to 1.0 if very strong wording implied).
        - PARTIAL => 0.5–0.6.
        - FAIL but commit count OK => 0.3–0.4.
        - FAIL plus sparse history => 0.0–0.2.
      If evidence ambiguous (few signals), default 0.5 not 0.2.
      Allowed: 0.0–1.0 step 0.1
      SCORE: <value>
      JUSTIFICATION: <reason>

  - id: commit_history
    name: "Proper commit history (not one big commit)"
    weight: 1
    prompt: |
      Use commit count + any analyzer git history notes.
        - Count >= min (3) and variety implied => 0.7–0.9.
        - Just meets threshold with little variety => 0.5–0.6.
        - Slightly below or barely passes with warning signals => 0.4–0.5.
        - Very sparse (1–2 commits) => 0.0–0.3.
      If analyzer did not provide log (edge), set 0.4 unless failure stated.
      Allowed: 0.0–1.0 step 0.1
      SCORE: <value>
      JUSTIFICATION: <reason>

  - id: invalid_inputs
    name: "Handles invalid inputs gracefully"
    weight: 1
    prompt: |
      Infer robustness indirectly (serializers, forms, validation hints).
        - Security + structure PASS plus mention of forms/serializers => 0.7–0.9.
        - Mixed PASS/PARTIAL with no explicit validation mention => 0.5–0.6.
        - Explicit FAILs on structure or security (could imply fragility) => 0.3–0.4.
        - Strong negative evidence (error-prone patterns) => 0.0–0.2.
      Lack of evidence alone: choose 0.5 (neutral).
      Allowed steps 0.1.
      SCORE: <value>
      JUSTIFICATION: <reason>

  - id: error_messages
    name: "Meaningful error messages instead of crashing"
    weight: 1
    prompt: |
      Infer from presence of Django/DRF patterns, serializers, validation, no crash indicators.
        - Strong structural PASS + no negative signals => 0.9–1.0.
        - Limited evidence but stable basics => 0.6–0.8.
        - Structural FAIL or complexity FAIL suggesting brittle error handling => 0.5–0.6.
        - Severe multiple FAIL (readability + complexity + security) => 0.0–0.4.
      Ambiguous: default 0.5.
      Allowed: 0.0–1.0 step 0.1
      SCORE: <value>
      JUSTIFICATION: <reason>

  - id: readme_detail
    name: "Well detailed README provided"
    weight: 1
    prompt: |
      Use README check (PASS/PARTIAL/FAIL) directly:
        - PASS + suggestions minor => 0.8–1.0.
        - PARTIAL (missing 1 element) => 0.5–0.7 depending on completeness.
        - FAIL but has at least one element (title or setup) => 0.3–0.4.
        - Completely absent / unusable => 0.0–0.2.
      If analyzer output truncated or unclear: 0.5 (neutral).
      Allowed: 0.0–1.0 step 0.1
      SCORE: <value>
      JUSTIFICATION: <reason>

  - id: backend_docs
    name: "Backend documentation provided (API / endpoints)"
    weight: 1
    prompt: |
      Judge from README analysis + any hints of endpoints or usage examples.
        - Explicit API/endpoint section or clear usage examples => 0.8–1.0.
        - README PASS but no explicit API section => 0.5–0.7.
        - README PARTIAL with vague references => 0.4–0.5.
        - README FAIL and no API hints => 0.0–0.3.
      If silent on APIs but otherwise structured: prefer 0.5 over very low (junior benefit of doubt).
      Allowed: 0.0–1.0 step 0.1
      SCORE: <value>
      JUSTIFICATION: <reason>

# Optional: manual overrides (create a JSON file with id->score to replace AI results)
# Not used by default; script will look for final_score_overrides.json if present.
